# üé® AI Image Generation Implementation Plan
**Project:** Twin up! Game
**Version:** 2.0 - Integrated AI Stack
**Date:** 2025-10-04
**Status:** Ready for Implementation

---

## üìã Executive Summary

### Current State
- ‚úÖ LocalAIProvider stub exists with ComfyUI integration code
- ‚úÖ ComfyUI Docker service defined but optional (--profile ai)
- ‚úÖ WebSocket progress tracking implemented
- ‚úÖ Mock AI provider active (`AI_PROVIDER=mock`)
- ‚ùå ComfyUI not integrated into main stack
- ‚ùå Sequential generation only (not parallel)
- ‚ùå Build fails due to model download issues

### Target State
- ‚úÖ **One Command Start:** `make start` or `./start.sh` launches everything including AI
- ‚úÖ **Parallel Generation:** Both team images generated simultaneously (3-5s total)
- ‚úÖ **Fully Local:** Zero API costs, works offline
- ‚úÖ **Production Ready:** Automatic failover, error handling, monitoring
- ‚úÖ **Fast:** <5 second total generation time for both images

### Key Constraints
- **Single Docker Stack:** All services start together with one command
- **Large Downloads:** ~5GB of models on first run (handle gracefully)
- **Parallel First:** Must implement parallel generation in Phase 1
- **Cross-Platform:** Works on macOS (Apple Silicon), Linux (x86/ARM), Windows

---

## üéØ Implementation Phases

### **Phase 1: Core Integration (PRIORITY 1)** ‚è±Ô∏è 2-3 hours

#### 1.1 Fix ComfyUI Docker Integration
**Status:** üî¥ Not Started
**Estimated Time:** 1 hour

**Problem:**
- ComfyUI currently behind `--profile ai` flag
- Docker build fails on Debian repo issues
- Models not downloaded during build

**Solution:**
Make ComfyUI a core service that starts automatically.

**Changes Required:**

##### A. Update docker-compose.yml
```yaml
services:
  # ... postgres, backend, frontend ...

  # ComfyUI AI Service - NOW ALWAYS ENABLED
  comfyui:
    build:
      context: ./ai-services
      dockerfile: Dockerfile.comfyui
      args:
        - DOWNLOAD_MODELS=true  # Download on first build
    container_name: ai-game-comfyui
    # REMOVED: profiles: - ai
    ports:
      - "8188:8188"
    volumes:
      - comfyui_models:/app/models
      - comfyui_output:/app/output
      - shared_images:/app/shared-images
    environment:
      - PYTORCH_ENABLE_MPS_FALLBACK=1
      - COMFYUI_MODEL_CACHE=/app/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8188/system_stats"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s  # Extended - models need to load
    networks:
      - ai-game-network
    restart: unless-stopped

  # Backend - NOW DEPENDS ON COMFYUI
  backend:
    # ... existing config ...
    environment:
      # ... existing vars ...
      - AI_PROVIDER=local  # Changed from 'mock' to 'local'
      - COMFYUI_HOST=http://comfyui:8188
      - ENABLE_PARALLEL_GENERATION=true
    depends_on:
      postgres:
        condition: service_healthy
      comfyui:
        condition: service_healthy  # Wait for ComfyUI to be ready
```

##### B. Create Optimized Dockerfile.comfyui
```dockerfile
# ai-services/Dockerfile.comfyui
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    wget \
    curl \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Clone ComfyUI (specific stable version)
RUN git clone https://github.com/comfyanonymous/ComfyUI.git . && \
    git checkout v0.1.1  # Use stable version

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt && \
    pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Create model directories
RUN mkdir -p models/checkpoints models/loras models/vae models/clip

# Download models (conditionally based on build arg)
ARG DOWNLOAD_MODELS=true
RUN if [ "$DOWNLOAD_MODELS" = "true" ]; then \
    # SD 1.5 Base Model (~4GB)
    wget -q --show-progress -O models/checkpoints/v1-5-pruned.safetensors \
        https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors && \
    # LCM LoRA for fast generation (~200MB)
    wget -q --show-progress -O models/loras/lcm-lora-sdv1-5.safetensors \
        https://huggingface.co/latent-consistency/lcm-lora-sdv1-5/resolve/main/pytorch_lora_weights.safetensors && \
    # VAE (~330MB)
    wget -q --show-progress -O models/vae/vae-ft-mse-840000-ema-pruned.safetensors \
        https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors && \
    echo "‚úÖ Models downloaded successfully"; \
    else \
    echo "‚ö†Ô∏è Skipping model download (use volumes for pre-downloaded models)"; \
    fi

# Create output directory
RUN mkdir -p output temp

# Health check script
RUN echo '#!/bin/sh\ncurl -f http://localhost:8188/system_stats || exit 1' > /health.sh && \
    chmod +x /health.sh

# Expose ComfyUI port
EXPOSE 8188

# Start ComfyUI
CMD ["python", "main.py", "--listen", "0.0.0.0", "--port", "8188"]
```

##### C. Update .dockerignore
```
# ai-services/.dockerignore
models/*
output/*
temp/*
*.pyc
__pycache__/
.git/
```

**Success Criteria:**
- ‚úÖ `make start` launches ComfyUI automatically
- ‚úÖ ComfyUI healthy within 3 minutes
- ‚úÖ `/system_stats` endpoint responds
- ‚úÖ Backend connects to ComfyUI successfully

---

#### 1.2 Implement Parallel Image Generation
**Status:** üî¥ Not Started
**Estimated Time:** 1 hour

**Problem:**
Current code generates images sequentially (Team A ‚Üí Team B)

**Solution:**
Generate both team images in parallel using Promise.all

##### A. Update roundService.ts
```typescript
// server/src/services/roundService.ts

import { generateImage } from './aiService';
import { TeamType } from '@prisma/client';

/**
 * Generate images for both teams in PARALLEL
 * This is the key optimization - both teams generate simultaneously
 */
export async function generateRoundImages(
  goodPrompt: string,
  evilPrompt: string,
  gameCode: string
): Promise<{ good: string; evil: string }> {

  console.log('üé® Starting PARALLEL image generation for both teams...');
  const startTime = Date.now();

  try {
    // Generate BOTH images at the same time
    const [goodImageUrl, evilImageUrl] = await Promise.all([
      generateImage(goodPrompt, {
        quality: 'standard',
        size: '1024x1024',
        team: TeamType.GOOD,
        gameCode
      }),
      generateImage(evilPrompt, {
        quality: 'standard',
        size: '1024x1024',
        team: TeamType.EVIL,
        gameCode
      })
    ]);

    const duration = Date.now() - startTime;
    const efficiency = duration < 8000 ? 'EXCELLENT' : duration < 12000 ? 'GOOD' : 'NEEDS IMPROVEMENT';

    console.log(`‚úÖ Parallel generation complete in ${duration}ms (${efficiency})`);
    console.log(`   Good team: ${goodImageUrl.substring(0, 60)}...`);
    console.log(`   Evil team: ${evilImageUrl.substring(0, 60)}...`);

    return {
      good: goodImageUrl,
      evil: evilImageUrl
    };

  } catch (error) {
    console.error('‚ùå Parallel image generation failed:', error);
    throw new Error(`Failed to generate round images: ${error}`);
  }
}

/**
 * Generate reference image for a round
 */
export async function generateRoundReferenceImage(): Promise<{
  imageUrl: string;
  prompt: string;
}> {
  const referencePrompts = [
    'A serene mountain landscape at sunset with vibrant colors',
    'A futuristic cityscape with flying vehicles and neon lights',
    'An underwater coral reef teeming with colorful fish',
    'A mystical forest with glowing mushrooms and fairy lights',
    'A cozy coffee shop on a rainy evening with warm lighting',
    'A space station orbiting a distant colorful planet',
    'A medieval castle on a cliff overlooking the ocean',
    'A vibrant street market with exotic fruits and spices',
    'A peaceful Japanese garden with koi pond and cherry blossoms',
    'A steampunk workshop filled with gears and inventions',
  ];

  const prompt = referencePrompts[Math.floor(Math.random() * referencePrompts.length)];

  console.log(`üé® Generating reference image: "${prompt}"`);

  const imageUrl = await generateImage(prompt, {
    quality: 'standard',
    size: '1024x1024'
  });

  return { imageUrl, prompt };
}
```

##### B. Update aiService.ts
```typescript
// server/src/services/aiService.ts

// Ensure LocalAIProvider supports parallel requests
export async function generateImage(
  prompt: string,
  options: {
    quality?: 'standard' | 'hd';
    size?: '1024x1024' | '1024x1792' | '1792x1024';
    team?: TeamType;
    gameCode?: string;
  } = {}
): Promise<string> {
  const { quality = 'standard', size = '1024x1024', team, gameCode } = options;

  // Route to Local AI Provider
  if (AI_PROVIDER === 'local' && localAI) {
    try {
      const available = await localAI.isAvailable();
      if (available) {
        // Pass team and gameCode for progress tracking
        return await localAI.generateImage(prompt, { quality, size }, team, gameCode);
      } else {
        console.warn('‚ö†Ô∏è ComfyUI not available, falling back to mock');
      }
    } catch (error: any) {
      console.error('‚ùå Local AI generation failed:', error.message);
      console.warn('‚ö†Ô∏è Falling back to mock image');
    }
  }

  // Fallback to mock
  if (USE_MOCK_AI || !openai) {
    console.log(`üé® [MOCK] Generating image for ${team || 'unknown'}: "${prompt.substring(0, 50)}..."`);
    const hash = simpleHash(prompt);
    return `https://picsum.photos/512/512?random=${hash}`;
  }

  // OpenAI fallback (if configured)
  // ... existing OpenAI code ...
}
```

##### C. Verify LocalAIProvider Concurrency
```typescript
// server/src/services/providers/localAIProvider.ts

export class LocalAIProvider {
  private activeGenerations: Map<string, Promise<string>>;

  constructor(io?: any) {
    // ... existing constructor ...
    this.activeGenerations = new Map(); // Track concurrent generations
  }

  async generateImage(
    prompt: string,
    options: ImageGenerationOptions = {},
    team?: TeamType,
    gameCode?: string
  ): Promise<string> {
    const generationKey = `${gameCode}-${team}`;

    // Check if already generating for this team
    if (this.activeGenerations.has(generationKey)) {
      console.warn(`‚ö†Ô∏è Generation already in progress for ${team} team`);
      return this.activeGenerations.get(generationKey)!;
    }

    const startTime = Date.now();
    console.log(`üé® [LocalAI] Starting generation for ${team || 'unknown'} team (parallel mode)`);

    // Create generation promise
    const generationPromise = (async () => {
      try {
        // Emit initial progress
        this.emitProgress(gameCode, team, 0, 4, `Initializing ${team} team...`);

        const [width, height] = this.parseSize(options.size || '1024x1024');
        const workflow = this.createWorkflow(prompt, width, height);

        // Queue prompt
        this.emitProgress(gameCode, team, 1, 4, `Queuing ${team} team...`);
        const promptId = await this.queuePrompt(workflow);

        // Generate
        this.emitProgress(gameCode, team, 2, 4, `Generating ${team} team image...`);
        const imageData = await this.waitForImage(promptId);

        // Save
        this.emitProgress(gameCode, team, 3, 4, `Saving ${team} team image...`);
        const imageUrl = await this.saveImage(imageData);

        // Complete
        this.emitProgress(gameCode, team, 4, 4, `${team} team complete!`);

        const duration = Date.now() - startTime;
        console.log(`‚úÖ [LocalAI] ${team} team generated in ${duration}ms`);

        return imageUrl;

      } catch (error) {
        console.error(`‚ùå [LocalAI] ${team} team failed:`, error);
        throw error;
      } finally {
        // Remove from active generations
        this.activeGenerations.delete(generationKey);
      }
    })();

    // Track this generation
    this.activeGenerations.set(generationKey, generationPromise);

    return generationPromise;
  }

  // ... rest of existing methods ...
}
```

**Success Criteria:**
- ‚úÖ Both team images generate simultaneously
- ‚úÖ Total time <8 seconds (target: 5 seconds)
- ‚úÖ WebSocket progress shows both teams updating in parallel
- ‚úÖ No race conditions or conflicts

---

#### 1.3 Update Frontend Progress UI
**Status:** üî¥ Not Started
**Estimated Time:** 30 minutes

**Problem:**
Existing `ImageGenerationLoader` component exists but may not show parallel progress clearly

**Solution:**
Ensure UI clearly shows both teams progressing simultaneously

##### A. Verify/Update ImageGenerationLoader.tsx
```typescript
// client/src/components/ImageGenerationLoader.tsx

import { useEffect, useState } from 'react';
import { useSocket } from '../hooks/useSocket';

interface ProgressState {
  step: number;
  totalSteps: number;
  message: string;
  percentage: number;
}

export function ImageGenerationLoader({ gameCode }: { gameCode: string }) {
  const { socket } = useSocket();
  const [progress, setProgress] = useState<{
    GOOD: ProgressState;
    EVIL: ProgressState;
  }>({
    GOOD: { step: 0, totalSteps: 4, message: 'Waiting...', percentage: 0 },
    EVIL: { step: 0, totalSteps: 4, message: 'Waiting...', percentage: 0 }
  });

  useEffect(() => {
    if (!socket) return;

    socket.on('image:progress', ({ team, step, totalSteps, message, percentage }) => {
      setProgress(prev => ({
        ...prev,
        [team]: { step, totalSteps, message, percentage }
      }));
    });

    return () => {
      socket.off('image:progress');
    };
  }, [socket]);

  return (
    <div className="w-full max-w-2xl mx-auto space-y-6 p-6 bg-white rounded-lg shadow-lg">
      <h2 className="text-2xl font-bold text-center mb-6">
        üé® Generating Images...
      </h2>

      <div className="space-y-4">
        <TeamProgress team="GOOD" progress={progress.GOOD} />
        <TeamProgress team="EVIL" progress={progress.EVIL} />
      </div>

      <div className="text-center text-sm text-gray-500 mt-4">
        ‚ö° Generating both images in parallel for maximum speed
      </div>
    </div>
  );
}

function TeamProgress({ team, progress }: { team: 'GOOD' | 'EVIL'; progress: ProgressState }) {
  const teamColor = team === 'GOOD' ? 'green' : 'red';
  const bgColor = team === 'GOOD' ? 'bg-green-100' : 'bg-red-100';
  const progressColor = team === 'GOOD' ? 'bg-green-500' : 'bg-red-500';
  const textColor = team === 'GOOD' ? 'text-green-700' : 'text-red-700';

  return (
    <div className={`${bgColor} p-4 rounded-lg transition-all duration-300`}>
      <div className="flex items-center justify-between mb-2">
        <div className="flex items-center gap-2">
          <span className={`font-bold ${textColor}`}>{team} Team</span>
          {progress.step === progress.totalSteps && (
            <span className="text-lg">‚úÖ</span>
          )}
        </div>
        <span className="text-sm font-mono">
          {progress.step}/{progress.totalSteps}
        </span>
      </div>

      {/* Progress Bar */}
      <div className="w-full bg-white rounded-full h-3 mb-2 overflow-hidden">
        <div
          className={`${progressColor} h-3 rounded-full transition-all duration-500 ease-out`}
          style={{ width: `${progress.percentage}%` }}
        />
      </div>

      {/* Status Message */}
      <p className="text-sm text-gray-700">
        {progress.message}
      </p>
    </div>
  );
}
```

**Success Criteria:**
- ‚úÖ Both team progress bars update simultaneously
- ‚úÖ Smooth animations
- ‚úÖ Clear visual feedback
- ‚úÖ Completion checkmarks appear when done

---

### **Phase 2: Optimization & Resilience (PRIORITY 2)** ‚è±Ô∏è 2-3 hours

#### 2.1 ComfyUI Workflow Optimization
**Status:** üî¥ Not Started
**Estimated Time:** 1 hour

**Goal:** Reduce generation time from 8-10s to 3-5s per image

**Optimizations:**

##### A. Use Faster Sampler Settings
```typescript
// localAIProvider.ts - createWorkflow()

private createWorkflow(prompt: string, width: number, height: number): any {
  const seed = Math.floor(Math.random() * 1000000000);

  return {
    "3": {
      "inputs": {
        "seed": seed,
        "steps": 4,           // Keep at 4 for LCM
        "cfg": 1.5,           // Low CFG for LCM
        "sampler_name": "lcm", // LCM sampler
        "scheduler": "lcm",    // LCM scheduler
        "denoise": 1,
        "model": ["11", 0],   // After LoRA loader
        "positive": ["6", 0],
        "negative": ["7", 0],
        "latent_image": ["5", 0]
      },
      "class_type": "KSampler"
    },
    "11": {  // LoRA Loader - ADDED
      "inputs": {
        "lora_name": "lcm-lora-sdv1-5.safetensors",
        "strength_model": 1.0,
        "strength_clip": 1.0,
        "model": ["4", 0],
        "clip": ["4", 1]
      },
      "class_type": "LoraLoader"
    },
    "4": {
      "inputs": {
        "ckpt_name": "v1-5-pruned.safetensors"
      },
      "class_type": "CheckpointLoaderSimple"
    },
    // ... rest of workflow
  };
}
```

##### B. Pre-warm Models on Startup
```typescript
// localAIProvider.ts

export class LocalAIProvider {
  private modelsPreloaded: boolean = false;

  async preloadModels(): Promise<void> {
    if (this.modelsPreloaded) return;

    try {
      console.log('üî• Pre-warming ComfyUI models...');
      const dummyPrompt = "test image for model preloading";
      const workflow = this.createWorkflow(dummyPrompt, 512, 512);

      await this.queuePrompt(workflow);
      await this.waitForImage(workflow.prompt_id, 30000);

      this.modelsPreloaded = true;
      console.log('‚úÖ Models preloaded and ready');
    } catch (error) {
      console.warn('‚ö†Ô∏è Model preload failed (will load on first use):', error);
    }
  }
}

// In aiService.ts initialization:
if (localAI) {
  localAI.preloadModels().catch(err =>
    console.warn('Model preload failed:', err)
  );
}
```

**Success Criteria:**
- ‚úÖ First generation after startup: 8-10s
- ‚úÖ Subsequent generations: 3-5s
- ‚úÖ Parallel generations complete in <8s total

---

#### 2.2 Graceful Degradation & Failover
**Status:** üî¥ Not Started
**Estimated Time:** 1 hour

**Goal:** Handle ComfyUI failures gracefully

##### A. Implement Health Check Monitoring
```typescript
// server/src/services/healthMonitor.ts

export class HealthMonitor {
  private healthy: boolean = false;
  private lastCheck: number = 0;
  private checkInterval: number = 30000; // 30s

  async checkHealth(provider: LocalAIProvider): Promise<boolean> {
    const now = Date.now();
    if (now - this.lastCheck < this.checkInterval) {
      return this.healthy;
    }

    this.lastCheck = now;
    this.healthy = await provider.isAvailable();

    if (!this.healthy) {
      console.warn('‚ö†Ô∏è ComfyUI health check failed');
    }

    return this.healthy;
  }
}
```

##### B. Add Retry Logic
```typescript
// aiService.ts

async function generateImageWithRetry(
  prompt: string,
  options: any,
  maxRetries: number = 2
): Promise<string> {
  let lastError: Error | null = null;

  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await generateImage(prompt, options);
    } catch (error) {
      lastError = error as Error;
      console.warn(`Generation attempt ${attempt + 1} failed:`, error);

      if (attempt < maxRetries - 1) {
        const delay = Math.pow(2, attempt) * 1000; // Exponential backoff
        console.log(`Retrying in ${delay}ms...`);
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
  }

  // Final fallback to mock
  console.error('All retry attempts failed, using mock image');
  const hash = simpleHash(prompt);
  return `https://picsum.photos/512/512?random=${hash}`;
}
```

**Success Criteria:**
- ‚úÖ Handles ComfyUI downtime gracefully
- ‚úÖ Retries failed generations
- ‚úÖ Falls back to mock if all else fails
- ‚úÖ Game never crashes due to AI failures

---

#### 2.3 Add Monitoring & Metrics
**Status:** üî¥ Not Started
**Estimated Time:** 1 hour

##### A. Track Generation Metrics
```typescript
// server/src/services/metricsService.ts

interface GenerationMetric {
  timestamp: number;
  provider: string;
  team?: string;
  durationMs: number;
  success: boolean;
  promptLength: number;
}

class MetricsCollector {
  private metrics: GenerationMetric[] = [];
  private readonly maxMetrics = 1000; // Keep last 1000

  record(metric: GenerationMetric) {
    this.metrics.push(metric);
    if (this.metrics.length > this.maxMetrics) {
      this.metrics.shift();
    }
  }

  getStats() {
    const successful = this.metrics.filter(m => m.success);
    const total = this.metrics.length;

    return {
      totalGenerations: total,
      successRate: successful.length / total,
      averageTime: successful.reduce((sum, m) => sum + m.durationMs, 0) / successful.length,
      last10Average: successful.slice(-10).reduce((sum, m) => sum + m.durationMs, 0) / 10,
      parallelEfficiency: this.calculateParallelEfficiency()
    };
  }

  private calculateParallelEfficiency(): number {
    // Compare parallel vs sequential times
    const parallelGenerations = this.metrics.filter(m => m.team);
    // Group by timestamp (same round)
    const rounds = new Map<number, GenerationMetric[]>();

    parallelGenerations.forEach(m => {
      const roundTime = Math.floor(m.timestamp / 60000); // 1min buckets
      if (!rounds.has(roundTime)) rounds.set(roundTime, []);
      rounds.get(roundTime)!.push(m);
    });

    let totalEfficiency = 0;
    let count = 0;

    rounds.forEach((metrics) => {
      if (metrics.length === 2) { // Both teams
        const maxTime = Math.max(...metrics.map(m => m.durationMs));
        const sumTime = metrics.reduce((sum, m) => sum + m.durationMs, 0);
        const efficiency = sumTime / maxTime; // Should be close to 2.0
        totalEfficiency += efficiency;
        count++;
      }
    });

    return count > 0 ? totalEfficiency / count : 0;
  }
}

export const metrics = new MetricsCollector();
```

##### B. Add Metrics Endpoint
```typescript
// server/src/routes/metrics.ts

import express from 'express';
import { metrics } from '../services/metricsService';

const router = express.Router();

router.get('/metrics', (_req, res) => {
  const stats = metrics.getStats();
  res.json(stats);
});

export default router;
```

**Success Criteria:**
- ‚úÖ Track all generations
- ‚úÖ Measure parallel efficiency
- ‚úÖ Monitor success rates
- ‚úÖ Expose metrics via API endpoint

---

### **Phase 3: Production Readiness (PRIORITY 3)** ‚è±Ô∏è 2-3 hours

#### 3.1 Update Makefile & Start Script
**Status:** üî¥ Not Started
**Estimated Time:** 30 minutes

##### A. Update Makefile
```makefile
# Makefile additions

# Start with AI (now default)
start:
	@echo "üöÄ Starting all services (including AI)..."
	./start.sh

# Build everything including AI models
build-all:
	@echo "üî® Building all services (this will download ~5GB of AI models)..."
	docker compose build

# AI-specific commands
ai-status:
	@echo "üìä ComfyUI Status:"
	@curl -s http://localhost:8188/system_stats | python3 -m json.tool || echo "‚ùå ComfyUI not available"

ai-test:
	@echo "üß™ Testing AI generation..."
	@curl -X POST http://localhost:3001/api/test/generate \
		-H "Content-Type: application/json" \
		-d '{"prompt":"a red apple"}' | python3 -m json.tool

ai-metrics:
	@echo "üìà AI Generation Metrics:"
	@curl -s http://localhost:3001/api/metrics | python3 -m json.tool

# Health check including AI
health:
	@echo "üè• Checking all services..."
	@echo ""
	@echo "Backend:"
	@curl -sf http://localhost:3001/api/health && echo "   ‚úÖ Healthy" || echo "   ‚ùå Down"
	@echo ""
	@echo "Frontend:"
	@curl -sf http://localhost/health && echo "   ‚úÖ Healthy" || echo "   ‚ùå Down"
	@echo ""
	@echo "Database:"
	@docker compose ps postgres | grep -q "healthy" && echo "   ‚úÖ Healthy" || echo "   ‚ùå Down"
	@echo ""
	@echo "ComfyUI:"
	@curl -sf http://localhost:8188/system_stats >/dev/null && echo "   ‚úÖ Healthy" || echo "   ‚ùå Down"
```

##### B. Update start.sh
```bash
#!/bin/bash

# ... existing header ...

# Check for first-time setup
if [ ! -f ".ai-models-downloaded" ]; then
    echo ""
    echo -e "${YELLOW}üì¶ First-time setup detected${NC}"
    echo -e "${YELLOW}This will download ~5GB of AI models. This only happens once.${NC}"
    echo -e "${YELLOW}Estimated time: 10-30 minutes depending on your internet speed.${NC}"
    echo ""
    read -p "Continue? (yes/no): " confirm
    if [ "$confirm" != "yes" ]; then
        echo -e "${RED}Setup cancelled${NC}"
        exit 1
    fi
fi

# ... existing build/start logic ...

# After successful start, mark models as downloaded
if [ ! -f ".ai-models-downloaded" ] && docker compose ps comfyui | grep -q "healthy"; then
    touch .ai-models-downloaded
    echo ""
    echo -e "${GREEN}‚úÖ AI models downloaded successfully!${NC}"
    echo -e "${GREEN}Future starts will be much faster.${NC}"
fi
```

**Success Criteria:**
- ‚úÖ `make start` works with AI included
- ‚úÖ First-time setup warns about download
- ‚úÖ `make ai-status` shows ComfyUI health
- ‚úÖ `make health` checks all services including AI

---

#### 3.2 Documentation Updates
**Status:** üî¥ Not Started
**Estimated Time:** 1 hour

##### A. Update QUICK_START.md
```markdown
# Quick Start

## First Time Setup

**Important:** The first time you run the application, it will download ~5GB of AI models.
This is a one-time download and will take 10-30 minutes depending on your internet speed.

```bash
# Clone and enter directory
git clone <repo-url>
cd AI-game

# First time: Build everything (downloads AI models)
make build-all

# Start all services including AI
make start
```

## System Requirements

- **RAM:** 8GB minimum, 16GB recommended
- **Disk:** 10GB free space (for AI models)
- **CPU:** Modern multi-core processor
- **GPU:** Optional (CPU mode works but slower)

## Services Included

When you run `make start`, you get:
- ‚úÖ Frontend (React)
- ‚úÖ Backend (Node.js + Express)
- ‚úÖ Database (PostgreSQL)
- ‚úÖ AI Service (ComfyUI + Stable Diffusion)
- ‚úÖ Cloudflare Tunnel (with `--quick-tunnel`)

## AI Image Generation

The game uses **local AI image generation** with Stable Diffusion:
- **Cost:** $0 per image
- **Speed:** ~3-5 seconds per image
- **Quality:** High quality, game-appropriate images
- **Privacy:** Everything runs locally, no data sent to cloud

### Performance

- **Parallel Generation:** Both team images generate simultaneously
- **First Generation:** 8-10 seconds (models loading)
- **Subsequent Generations:** 3-5 seconds
- **Fallback:** Automatically uses mock images if AI fails
```

##### B. Create AI_TROUBLESHOOTING.md
```markdown
# AI Service Troubleshooting

## ComfyUI Won't Start

**Symptoms:**
- `make health` shows ComfyUI as unhealthy
- Timeout waiting for ComfyUI
- Container exits immediately

**Solutions:**

1. Check logs:
```bash
make ai-logs
```

2. Verify models downloaded:
```bash
docker compose exec comfyui ls -lh models/checkpoints/
```

3. Rebuild with fresh models:
```bash
docker compose down -v
rm .ai-models-downloaded
make build-all
```

## Slow Generation Times

**If generation takes >10 seconds:**

1. Check CPU usage (should be high during generation)
2. Ensure no other heavy processes running
3. Consider using GPU if available

## Out of Memory Errors

**Reduce memory usage:**

```yaml
# docker-compose.yml - Add resource limits
comfyui:
  deploy:
    resources:
      limits:
        memory: 4G  # Adjust based on your system
```

## Models Download Fails

**If model download fails:**

1. Check internet connection
2. Try downloading manually:
```bash
docker compose exec comfyui bash
wget -O models/checkpoints/v1-5-pruned.safetensors \
  https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors
```
```

**Success Criteria:**
- ‚úÖ Clear setup instructions
- ‚úÖ Troubleshooting guide
- ‚úÖ Performance expectations documented
- ‚úÖ System requirements listed

---

#### 3.3 Integration Testing
**Status:** üî¥ Not Started
**Estimated Time:** 1.5 hours

##### A. Create Test Suite
```typescript
// server/tests/integration/aiGeneration.test.ts

describe('AI Image Generation - Integration Tests', () => {

  beforeAll(async () => {
    // Ensure ComfyUI is running
    const available = await localAI?.isAvailable();
    if (!available) {
      throw new Error('ComfyUI must be running for integration tests');
    }
  });

  describe('Single Image Generation', () => {
    it('should generate image in <10 seconds', async () => {
      const start = Date.now();
      const imageUrl = await generateImage('a red apple on a wooden table');
      const duration = Date.now() - start;

      expect(imageUrl).toMatch(/^http/);
      expect(duration).toBeLessThan(10000);
    });

    it('should handle complex prompts', async () => {
      const prompt = 'A mystical forest with glowing mushrooms, fireflies, and a crystal-clear stream under moonlight';
      const imageUrl = await generateImage(prompt);

      expect(imageUrl).toBeDefined();
    });
  });

  describe('Parallel Generation', () => {
    it('should generate both images in parallel', async () => {
      const start = Date.now();

      const [img1, img2] = await Promise.all([
        generateImage('a sunny beach', { team: TeamType.GOOD }),
        generateImage('a stormy ocean', { team: TeamType.EVIL })
      ]);

      const duration = Date.now() - start;

      expect(img1).toBeDefined();
      expect(img2).toBeDefined();
      expect(duration).toBeLessThan(12000); // Both under 12s
    });

    it('should achieve >1.5x parallel efficiency', async () => {
      // Measure sequential
      const seqStart = Date.now();
      await generateImage('test 1');
      await generateImage('test 2');
      const seqDuration = Date.now() - seqStart;

      // Measure parallel
      const parStart = Date.now();
      await Promise.all([
        generateImage('test 3'),
        generateImage('test 4')
      ]);
      const parDuration = Date.now() - parStart;

      const efficiency = seqDuration / parDuration;
      expect(efficiency).toBeGreaterThan(1.5); // At least 1.5x speedup
    });
  });

  describe('Error Handling', () => {
    it('should retry on failure', async () => {
      // Mock a temporary failure
      jest.spyOn(localAI!, 'queuePrompt').mockRejectedValueOnce(new Error('Network error'));

      const imageUrl = await generateImageWithRetry('test prompt');
      expect(imageUrl).toBeDefined();
    });

    it('should fallback to mock on complete failure', async () => {
      jest.spyOn(localAI!, 'isAvailable').mockResolvedValue(false);

      const imageUrl = await generateImage('test prompt');
      expect(imageUrl).toMatch(/picsum.photos/);
    });
  });

  describe('Progress Tracking', () => {
    it('should emit progress events', (done) => {
      const mockSocket = {
        to: jest.fn().mockReturnThis(),
        emit: jest.fn()
      };

      // Inject mock socket
      (global as any).io = mockSocket;

      generateImage('test', { team: TeamType.GOOD, gameCode: 'TEST123' });

      setTimeout(() => {
        expect(mockSocket.emit).toHaveBeenCalledWith('image:progress',
          expect.objectContaining({
            team: TeamType.GOOD,
            step: expect.any(Number),
            message: expect.any(String)
          })
        );
        done();
      }, 1000);
    });
  });
});
```

##### B. Add E2E Test
```bash
# tests/e2e/full-game-flow.sh

#!/bin/bash

echo "üß™ Running E2E Test: Full Game Flow with AI"

# 1. Start services
echo "Starting services..."
make start
sleep 30

# 2. Create game
GAME_RESPONSE=$(curl -s -X POST http://localhost:3001/api/games \
  -H "Content-Type: application/json" \
  -d '{"hostName":"TestHost","totalRounds":1}')

GAME_CODE=$(echo $GAME_RESPONSE | jq -r '.game.code')
echo "‚úÖ Game created: $GAME_CODE"

# 3. Add players
for i in {1..4}; do
  curl -s -X POST http://localhost:3001/api/games/$GAME_CODE/join \
    -H "Content-Type: application/json" \
    -d "{\"playerName\":\"Player$i\"}" > /dev/null
  echo "‚úÖ Player $i joined"
done

# 4. Start game
curl -s -X POST http://localhost:3001/api/games/$GAME_CODE/start > /dev/null
echo "‚úÖ Game started"

# 5. Submit prompts
ROUND_ID=$(curl -s http://localhost:3001/api/games/$GAME_CODE | jq -r '.game.currentRound.id')

curl -s -X POST http://localhost:3001/api/rounds/$ROUND_ID/submit \
  -H "Content-Type: application/json" \
  -d '{"playerId":"player1","prompt":"a beautiful sunset"}' > /dev/null

curl -s -X POST http://localhost:3001/api/rounds/$ROUND_ID/submit \
  -H "Content-Type: application/json" \
  -d '{"playerId":"player2","prompt":"a stormy night"}' > /dev/null

echo "‚úÖ Prompts submitted"

# 6. Wait for image generation
echo "‚è≥ Waiting for parallel AI generation..."
sleep 15

# 7. Verify images generated
ROUND_DATA=$(curl -s http://localhost:3001/api/rounds/$ROUND_ID)
GOOD_IMAGE=$(echo $ROUND_DATA | jq -r '.round.goodTeamImage')
EVIL_IMAGE=$(echo $ROUND_DATA | jq -r '.round.evilTeamImage')

if [[ "$GOOD_IMAGE" =~ ^http ]] && [[ "$EVIL_IMAGE" =~ ^http ]]; then
  echo "‚úÖ Both images generated successfully!"
  echo "   Good team: $GOOD_IMAGE"
  echo "   Evil team: $EVIL_IMAGE"
  exit 0
else
  echo "‚ùå Image generation failed"
  exit 1
fi
```

**Success Criteria:**
- ‚úÖ All unit tests pass
- ‚úÖ Integration tests verify parallel generation
- ‚úÖ E2E test completes full game flow
- ‚úÖ Parallel efficiency >1.5x measured

---

## üìä Success Metrics & KPIs

| Metric | Target | Measurement Method | Status |
|--------|--------|-------------------|--------|
| **Parallel Generation Time** | <8 seconds | Integration test timing | üî¥ |
| **Single Image Time** | <5 seconds | Integration test timing | üî¥ |
| **First-Time Setup** | <30 minutes | Manual test | üî¥ |
| **Parallel Efficiency** | >1.7x speedup | Metrics API | üî¥ |
| **Success Rate** | >95% | Metrics API | üî¥ |
| **Memory Usage** | <4GB | Docker stats | üî¥ |
| **Startup Time** | <3 minutes | Docker compose up timing | üî¥ |

---

## üö® Known Risks & Mitigation

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| **Model download fails** | HIGH | MEDIUM | Manual download script, cached volumes |
| **Out of memory** | HIGH | MEDIUM | Resource limits, reduce batch size |
| **Slow generation** | HIGH | LOW | Use LCM LoRA, optimize workflow |
| **ComfyUI crashes** | MEDIUM | LOW | Automatic restart, fallback to mock |
| **First-time setup too slow** | LOW | HIGH | Clear expectations, progress indicators |

---

## üìÖ Implementation Timeline

### Day 1 (3-4 hours)
- ‚úÖ Phase 1.1: Fix ComfyUI Docker (1h)
- ‚úÖ Phase 1.2: Implement Parallel Generation (1h)
- ‚úÖ Phase 1.3: Update Frontend UI (30min)
- ‚úÖ Test end-to-end flow (1h)

### Day 2 (2-3 hours)
- ‚úÖ Phase 2.1: Workflow Optimization (1h)
- ‚úÖ Phase 2.2: Failover & Resilience (1h)
- ‚úÖ Phase 2.3: Metrics (1h)

### Day 3 (2-3 hours)
- ‚úÖ Phase 3.1: Makefile updates (30min)
- ‚úÖ Phase 3.2: Documentation (1h)
- ‚úÖ Phase 3.3: Testing (1.5h)

**Total Estimated Time:** 7-10 hours

---

## üéØ Implementation Checklist

### Phase 1: Core Integration ‚úÖ
- [ ] Remove `--profile ai` from docker-compose.yml
- [ ] Update ComfyUI Dockerfile with model downloads
- [ ] Change `AI_PROVIDER=mock` to `AI_PROVIDER=local`
- [ ] Add backend dependency on ComfyUI health
- [ ] Implement parallel generation in roundService.ts
- [ ] Update aiService.ts for concurrency support
- [ ] Verify LocalAIProvider handles parallel requests
- [ ] Update ImageGenerationLoader.tsx UI
- [ ] Test parallel progress display

### Phase 2: Optimization ‚úÖ
- [ ] Add LoRA loader to workflow
- [ ] Implement model preloading
- [ ] Add health monitoring
- [ ] Implement retry logic
- [ ] Add metrics collection
- [ ] Create metrics API endpoint
- [ ] Test parallel efficiency measurement

### Phase 3: Production Ready ‚úÖ
- [ ] Update Makefile with AI commands
- [ ] Update start.sh with first-time warning
- [ ] Update QUICK_START.md
- [ ] Create AI_TROUBLESHOOTING.md
- [ ] Write integration tests
- [ ] Create E2E test script
- [ ] Verify all metrics meet targets
- [ ] Load test with multiple concurrent games

---

## üîß Environment Variables

```bash
# AI Configuration
AI_PROVIDER=local                    # 'local', 'openai', or 'mock'
COMFYUI_HOST=http://comfyui:8188    # ComfyUI endpoint
ENABLE_PARALLEL_GENERATION=true      # Enable parallel mode
IMAGE_URL_BASE=http://localhost:3001/images

# Performance
COMFYUI_TIMEOUT=60000               # 60s timeout
MAX_CONCURRENT_GENERATIONS=2        # Parallel limit
ENABLE_MODEL_PRELOAD=true           # Preload on startup

# Fallback
USE_MOCK_AI=false                   # Mock fallback
OPENAI_API_KEY=                     # OpenAI fallback (optional)
```

---

## üìö Additional Resources

- **ComfyUI Docs:** https://github.com/comfyanonymous/ComfyUI
- **LCM LoRA:** https://huggingface.co/latent-consistency/lcm-lora-sdv1-5
- **Stable Diffusion:** https://github.com/Stability-AI/stablediffusion
- **Docker Compose:** https://docs.docker.com/compose/

---

## ‚úÖ Definition of Done

This implementation is considered complete when:

1. ‚úÖ `make start` launches all services including AI in one command
2. ‚úÖ Both team images generate in parallel within 8 seconds total
3. ‚úÖ Frontend shows real-time progress for both teams simultaneously
4. ‚úÖ ComfyUI handles 2+ concurrent generations without issues
5. ‚úÖ Graceful fallback to mock images if AI fails
6. ‚úÖ First-time setup completes successfully with clear progress
7. ‚úÖ All integration tests pass with >95% success rate
8. ‚úÖ Documentation is complete and accurate
9. ‚úÖ Parallel efficiency measured at >1.7x speedup
10. ‚úÖ Game night tested with real users successfully

---

**Status:** üî¥ Ready for Implementation
**Next Action:** Begin Phase 1.1 - Fix ComfyUI Docker Integration
**Owner:** Development Team
**Last Updated:** 2025-10-04
